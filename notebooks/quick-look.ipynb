{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rubicon Quick Look\n",
    "\n",
    "This is a simple classification example based \n",
    "on this [`scikit-learn` example](https://scikit-learn.org/stable/auto_examples/preprocessing/plot_scaling_importance.html#sphx-glr-auto-examples-preprocessing-plot-scaling-importance-py).\n",
    "The wine dataset used in this example classifies different Italian wines by the cultivators that produced them based \n",
    "on 13 features describing their chemical makeup. More infomation on this dataset can be found by running \n",
    "`print(wine.DESCR)` after the dataset has been loaded.\n",
    "\n",
    "The pipeline contains an optional `StandardScaler` configured by the `is_standardized` parameter, a `PCA` \n",
    "configured by the `n_components` parameter, and finally a `GaussianNB`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "wine = load_wine()\n",
    "X_train, X_test, y_train, y_test = train_test_split(wine.data, wine.target, test_size=0.50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rubicon import Rubicon\n",
    "\n",
    "root_dir = \"./rubicon-root\"\n",
    "\n",
    "rubicon = Rubicon(persistence=\"filesystem\", root_dir=root_dir)\n",
    "project = rubicon.get_or_create_project(\"Wine Classification\")\n",
    "\n",
    "print(project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def run_experiment(project, is_standardized=False, n_components=2):\n",
    "    experiment = project.log_experiment(model_name=GaussianNB.__name__)\n",
    "    \n",
    "    experiment.log_parameter(\"is_standardized\", is_standardized)\n",
    "    experiment.log_parameter(\"n_components\", n_components)\n",
    "\n",
    "    for name in wine.feature_names:\n",
    "        experiment.log_feature(name)\n",
    "\n",
    "    if is_standardized:\n",
    "        classifier = make_pipeline(StandardScaler(), PCA(n_components=n_components), GaussianNB())\n",
    "    else:\n",
    "        classifier = make_pipeline(PCA(n_components=n_components), GaussianNB())\n",
    "        \n",
    "    classifier.fit(X_train, y_train)\n",
    "    pred_test = classifier.predict(X_test)\n",
    "    accuracy = metrics.accuracy_score(y_test, pred_test)\n",
    "    \n",
    "    experiment.log_metric(\"accuracy\", accuracy)\n",
    "    \n",
    "    confusion_matrix = pd.crosstab(\n",
    "        wine.target_names[y_test], wine.target_names[pred_test],\n",
    "        rownames=[\"actual\"], colnames=[\"predicted\"],\n",
    "    )\n",
    "    \n",
    "    experiment.log_dataframe(\n",
    "        confusion_matrix, tags=[\"confusion matrix\"]\n",
    "    )\n",
    "\n",
    "    if accuracy >= .9:\n",
    "        experiment.add_tags([\"success\"])\n",
    "    else:\n",
    "        experiment.add_tags([\"failure\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the `multiprocessing` package to run 14 uniquely configured experiments in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "processes = []\n",
    "\n",
    "for is_standardized in [True, False]:\n",
    "    for n_components in range(1, 15, 2):\n",
    "        processes.append(multiprocessing.Process(\n",
    "            target=run_experiment, args=[project],\n",
    "            kwargs={\"is_standardized\": is_standardized, \"n_components\": n_components}\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for process in processes:\n",
    "    process.start()\n",
    "    \n",
    "for process in processes:\n",
    "    process.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S3 Logging\n",
    "\n",
    "Logging to S3 is as simple as changing the `root_dir` when instantiating the `Rubicon` object:\n",
    "\n",
    "```python\n",
    "rubicon = Rubicon(persistence=\"filesystem\", root_dir=\"s3://my-bucket/path/to/rubicon-root\")\n",
    "```\n",
    "\n",
    "`rubicon` can even help you sync a locally logged project to S3 after the fact:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_path = \"s3://my-bucket/path/to/rubicon-root\"\n",
    "rubicon.sync(project.name, s3_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create a second `rubicon` client pointing to S3 to verify our experiments were copied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rubicon_s3 = Rubicon(persistence=\"filesystem\", root_dir=s3_path)\n",
    "project_s3 = rubicon_s3.get_project(\"Wine Classification\")\n",
    "\n",
    "assert len(project_s3.experiments()) == len(project.experiments())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Publishing & Sharing\n",
    "\n",
    "Once we've synced our local project with S3, anyone with access to that bucket and knowledge of how to use the `rubicon` library can use it to pull the project and explore the data themselves.\n",
    "\n",
    "But, we've also made it even easier for users that aren't familiar with `rubicon`! Collaborators and reviewers can use [**Intake**](https://intake.readthedocs.io/en/latest/), an open source library whose goal is *\\\"taking the pain out of data access and distribution\\\"*, to load published `rubicon` experiments locally.\n",
    "\n",
    "First, we'll \"publish\" some `rubicon` experiments by generating an Intake catalog that we can share. \"Publishing\" a project or experiment does not change or move the data, it is simply a way to designate an object as finalized. A **catalog** is a single YAML file that works as a pointer to the actual Rubicon data. Sharing and versioning this one file is much simpler than doing the same with an entire directory of data.\n",
    "\n",
    "**Note**: We're using the **S3 client** we just created to publish here. Sharing a catalog that points to local files on your own machine isn't going to be any help unless you're certain your collaborators will be working on the same machine!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rubicon_s3.publish(project_s3.name, experiment_tags=[\"success\"], output_filepath=\"./wine_catalog.yml\")\n",
    "\n",
    "! ls -l wine_catalog.yml\n",
    "! cat wine_catalog.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consuming Published Data\n",
    "\n",
    "As a consumer of Rubicon data, reading the catalog and loading the published Rubicon experiments into memory only requires the Intake drivers included in `rubicon.intake-rubicon`. Intake was installed as a part of the `rubicon` library, so we can use it directly to load our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import intake\n",
    "\n",
    "catalog = intake.open_catalog(\"./wine_catalog.yml\")\n",
    "list(catalog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out the collaboration and review example notebook for more details on reading catalogs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dashboard\n",
    "\n",
    "If youâ€™ve installed the ui extra, you can use the **CLI** launch the Rubicon Dashboard locally to explore your logged data. The dashboard is fully interactive and offers a number of ways to compare your experiments, with even more on the way! (Interrupt the kernel from the menu above to stop running the dashboard.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rubicon ui --root-dir \"./rubicon-root\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
