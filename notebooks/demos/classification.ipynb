{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afee719a-8aef-45cd-9ede-d8afaaa2641e",
   "metadata": {},
   "source": [
    "# Hello `rubicon-ml`!\n",
    "\n",
    "This notebook highlights the main features of the `rubicon-ml` logging library\n",
    "in the context of a classification problem. `rubicon-ml` can track model metadata\n",
    "and artifacts to aid in reproducible experimentation.\n",
    "\n",
    "## loading & preparing the data\n",
    "\n",
    "First, let's take a look at the data we'll be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f601daf-09a3-43e3-a234-4bb90fe95974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "! pip install palmerpenguins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453df8b7-bcee-4ef3-bec6-b6688ee786e5",
   "metadata": {},
   "source": [
    "More information on the palmer pengunis dataset can be found\n",
    "[here](https://allisonhorst.github.io/palmerpenguins/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b842175c-3949-48ef-b1e3-bb0cdd35fafc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>species</th>\n",
       "      <th>island</th>\n",
       "      <th>bill_length_mm</th>\n",
       "      <th>bill_depth_mm</th>\n",
       "      <th>flipper_length_mm</th>\n",
       "      <th>body_mass_g</th>\n",
       "      <th>sex</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>39.1</td>\n",
       "      <td>18.7</td>\n",
       "      <td>181.0</td>\n",
       "      <td>3750.0</td>\n",
       "      <td>male</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>39.5</td>\n",
       "      <td>17.4</td>\n",
       "      <td>186.0</td>\n",
       "      <td>3800.0</td>\n",
       "      <td>female</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>40.3</td>\n",
       "      <td>18.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>3250.0</td>\n",
       "      <td>female</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>36.7</td>\n",
       "      <td>19.3</td>\n",
       "      <td>193.0</td>\n",
       "      <td>3450.0</td>\n",
       "      <td>female</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  species     island  bill_length_mm  bill_depth_mm  flipper_length_mm  \\\n",
       "0  Adelie  Torgersen            39.1           18.7              181.0   \n",
       "1  Adelie  Torgersen            39.5           17.4              186.0   \n",
       "2  Adelie  Torgersen            40.3           18.0              195.0   \n",
       "3  Adelie  Torgersen             NaN            NaN                NaN   \n",
       "4  Adelie  Torgersen            36.7           19.3              193.0   \n",
       "\n",
       "   body_mass_g     sex  year  \n",
       "0       3750.0    male  2007  \n",
       "1       3800.0  female  2007  \n",
       "2       3250.0  female  2007  \n",
       "3          NaN     NaN  2007  \n",
       "4       3450.0  female  2007  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from palmerpenguins import load_penguins\n",
    "\n",
    "penguins_df = load_penguins()\n",
    "penguins_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acca1493-61cb-43be-a354-b5929adf41cf",
   "metadata": {},
   "source": [
    "We can validate that we're looking at the right data by comparing this plot to the\n",
    "one at the link above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d5841f3-add3-495d-a68f-2035df60335c",
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "px.scatter(penguins_df, x=\"flipper_length_mm\", y=\"bill_length_mm\", color=\"species\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fabe92-c386-4b1d-b32f-f231a64ef04c",
   "metadata": {},
   "source": [
    "Before we get started, we'll encode the catagorical variables in the palmer penguins\n",
    "dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83c9cae9-8746-43ab-80b4-70b625dd0340",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>species</th>\n",
       "      <th>island</th>\n",
       "      <th>bill_length_mm</th>\n",
       "      <th>bill_depth_mm</th>\n",
       "      <th>flipper_length_mm</th>\n",
       "      <th>body_mass_g</th>\n",
       "      <th>sex</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>39.1</td>\n",
       "      <td>18.7</td>\n",
       "      <td>181.0</td>\n",
       "      <td>3750.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>39.5</td>\n",
       "      <td>17.4</td>\n",
       "      <td>186.0</td>\n",
       "      <td>3800.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>40.3</td>\n",
       "      <td>18.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>3250.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>36.7</td>\n",
       "      <td>19.3</td>\n",
       "      <td>193.0</td>\n",
       "      <td>3450.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   species  island  bill_length_mm  bill_depth_mm  flipper_length_mm  \\\n",
       "0        0       2            39.1           18.7              181.0   \n",
       "1        0       2            39.5           17.4              186.0   \n",
       "2        0       2            40.3           18.0              195.0   \n",
       "3        0       2             NaN            NaN                NaN   \n",
       "4        0       2            36.7           19.3              193.0   \n",
       "\n",
       "   body_mass_g  sex  year  \n",
       "0       3750.0    1  2007  \n",
       "1       3800.0    0  2007  \n",
       "2       3250.0    0  2007  \n",
       "3          NaN    2  2007  \n",
       "4       3450.0    0  2007  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "penguin_encoder = LabelEncoder()\n",
    "\n",
    "for column in [\"species\", \"island\", \"sex\"]:\n",
    "    penguins_df[column] = penguin_encoder.fit_transform(penguins_df[column])\n",
    "\n",
    "penguins_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d548ce66-41d6-41ab-9f04-a10c7e3a8a3e",
   "metadata": {},
   "source": [
    "## setting up a pipeline\n",
    "\n",
    "We'll be leveraging Scikit-learn quite a bit in this example, but `rubicon-ml` can\n",
    "be integrated into any Python codebase! Let's split the palmer penguins dataset into\n",
    "a train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0559e9ec-bf60-43b0-9fd3-1ef2adea24e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((240, 7), (240,), (104, 7), (104,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_penguins_df, test_penguins_df = train_test_split(penguins_df, test_size=.30)\n",
    "\n",
    "target_column = \"species\"\n",
    "feature_columns = [c for c in train_penguins_df.columns if c != target_column]\n",
    "\n",
    "X_train, y_train = train_penguins_df[feature_columns], train_penguins_df[target_column]\n",
    "X_test, y_test = test_penguins_df[feature_columns], test_penguins_df[target_column]\n",
    "\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f372b5c9-d276-41be-b741-6009dd8949d3",
   "metadata": {},
   "source": [
    "Now we can build a simple pipeline to train a classifier on the palmer penguin data.\n",
    "We'll start with an imputer to fill out some missing data then train a k-nearest\n",
    "neighbors classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85ac590f-47f4-4f72-b323-a7edf7bfe0a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7788461538461539"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "steps = [\n",
    "    (\"si\", SimpleImputer(strategy=\"mean\")),\n",
    "    (\"kn\", KNeighborsClassifier(n_neighbors=5)),\n",
    "]\n",
    "\n",
    "penguin_pipeline = Pipeline(steps=steps)\n",
    "penguin_pipeline.fit(X_train, y_train)\n",
    "\n",
    "score = penguin_pipeline.score(X_test, y_test)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2baaca03-0d6b-45e7-8f07-1a310c8ba95e",
   "metadata": {},
   "source": [
    "## logging metadata to `rubicon-ml`\n",
    "\n",
    "Now that we've fit our pipeline, let's log our inputs and outputs to `rubicon-ml`\n",
    "so we can revisit them later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b159dd1-598e-405c-9ea7-a5e8b36c07d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rubicon_ml import Rubicon\n",
    "\n",
    "rubicon = Rubicon(\n",
    "    persistence=\"filesystem\",\n",
    "    root_dir=\"./rubicon-root\",\n",
    "    auto_git_enabled=True,\n",
    ")\n",
    "project = rubicon.get_or_create_project(name=\"demo\")\n",
    "\n",
    "experiment = project.log_experiment(name=\"classifying penguins\")\n",
    "parameter_strategy = experiment.log_parameter(name=\"strategy\", value=\"mean\")\n",
    "parameter_n_neighbors = experiment.log_parameter(name=\"n_neighbors\", value=5)\n",
    "metric_accuracy = experiment.log_metric(name=\"accuracy\", value=score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e745a558-e013-4eb2-8232-85dc52d90016",
   "metadata": {},
   "source": [
    "Users can retrieve parameters, metrics, and lots of other metadata from logged experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1026126c-f680-4610-914f-cdeff9f1f9f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment(project_name='demo', id='8dd488f3-7dd7-4ad5-ac82-221e5f6db900', name='classifying penguins', description=None, model_name=None, branch_name='example', commit_hash='b66a307cfb99ea91ec2f11b6fdd16ce41e3c54ab', training_metadata=None, tags=[], created_at=datetime.datetime(2022, 3, 29, 18, 36, 1, 48644))\n",
      "example b66a307cfb99ea91ec2f11b6fdd16ce41e3c54ab\n",
      "\n",
      "[('strategy', 'mean'), ('n_neighbors', 5)]\n",
      "[('accuracy', 0.7788461538461539)]\n"
     ]
    }
   ],
   "source": [
    "print(experiment)\n",
    "print(experiment.branch_name, experiment.commit_hash)\n",
    "print()\n",
    "print([(p.name, p.value) for p in experiment.parameters()])\n",
    "print([(m.name, m.value) for m in experiment.metrics()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196d5b3c-c603-4f37-8c4d-7148f333b0f9",
   "metadata": {},
   "source": [
    "## logging with the `RubiconPipeline`\n",
    "\n",
    "The experiment tracking code above can be easily inserted into any Python codebase with a little\n",
    "bit of thought around what you want to be logging. However, when using Scikit-learn, `rubicon-ml`\n",
    "can do a little bit of that thinking for you!\n",
    "\n",
    "The `RubiconPipeline` is a drop-in replacement for the existing Scikit-learn `Pipeline` we used above.\n",
    "However, the `RubiconPipeline` is able to intelligently log a new experiment that contains each input \n",
    "to the pipeline every time the pipeline is fit. Scoring the pipeline will also log metrics.\n",
    "\n",
    "Here, we'll simply instantiate a `RubiconPipeline` with the same steps we used above - but this time\n",
    "we'll give it a `rubicon-ml` project too!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "442e9ac4-b9c0-4c55-ae9e-8f58028670d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7788461538461539"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rubicon_ml.sklearn import RubiconPipeline\n",
    "\n",
    "rubicon_penguin_pipeline = RubiconPipeline(\n",
    "    project=project,\n",
    "    experiment_kwargs={\"name\": \"KNeighborsClassifier\", \"tags\": [\"knn\"]},\n",
    "    steps=steps,\n",
    ")\n",
    "rubicon_penguin_pipeline.fit(X_train, y_train)\n",
    "\n",
    "pipeline_experiment = rubicon_penguin_pipeline.experiment\n",
    "\n",
    "rubicon_penguin_pipeline.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c08b52-0bff-4cc0-8042-78c5daea2928",
   "metadata": {},
   "source": [
    "After fitting and scoring the new `RubiconPipeline`, we can clearly see that we've captured\n",
    "a lot more information than the manually logged pipeline with far less code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17b1eee2-f7bc-498f-b751-4daf9baf48fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment(project_name='demo', id='8aa5f5a0-3448-4ce3-8124-e8858b4d5bc1', name='KNeighborsClassifier', description=None, model_name=None, branch_name='example', commit_hash='b66a307cfb99ea91ec2f11b6fdd16ce41e3c54ab', training_metadata=None, tags=['knn'], created_at=datetime.datetime(2022, 3, 29, 18, 36, 3, 798132))\n",
      "\n",
      "[('si__add_indicator', False), ('si__copy', True), ('si__fill_value', None), ('si__missing_values', nan), ('si__strategy', 'mean'), ('si__verbose', 0), ('kn__algorithm', 'auto'), ('kn__leaf_size', 30), ('kn__metric', 'minkowski'), ('kn__metric_params', None), ('kn__n_jobs', None), ('kn__n_neighbors', 5), ('kn__p', 2), ('kn__weights', 'uniform')]\n",
      "[('score', 0.7788461538461539)]\n"
     ]
    }
   ],
   "source": [
    "print(pipeline_experiment)\n",
    "print()\n",
    "print([(p.name, p.value) for p in pipeline_experiment.parameters()])\n",
    "print([(m.name, m.value) for m in pipeline_experiment.metrics()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85ac407-5b30-4bd3-a43a-11c944383c46",
   "metadata": {},
   "source": [
    "## grid search (and other meta-estimators)\n",
    "\n",
    "The `RubiconPipeline` itself is a Scikit-learn estimator, meaning that it can be leveraged\n",
    "with any existing meta-estimator that Scikit-learn or a variety of other tooling suites\n",
    "offer. The great part about integrating `rubicon-ml`Â into Scikit-learn is that so many other\n",
    "tools have also decided to make the same integration, effectively giving `rubicon-ml` compatibility\n",
    "with them as well.\n",
    "\n",
    "`rubcion-ml` is particularly useful in evaluating hyperparamter- and feature-space searches while using\n",
    "tools that levearage the Scikit-learn estimator specification. For this particular example, we'll use\n",
    "the built-in `GridSearchCV`, but `rubicon-ml` can work with any searching tool that levearges the\n",
    "Scikit-learn API like Optuna or Hyperopt.\n",
    "\n",
    "The grid search below will train 36 separate models and automatically log an experiment representing\n",
    "each of them to the newly created \"grid search demo\" project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "173fad54-1e4f-4d2d-9b7a-3bd773610e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 18 candidates, totalling 36 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<rubicon_ml.client.experiment.Experiment at 0x1609a2020>,\n",
       " <rubicon_ml.client.experiment.Experiment at 0x1609a14e0>,\n",
       " <rubicon_ml.client.experiment.Experiment at 0x1609a1c60>,\n",
       " <rubicon_ml.client.experiment.Experiment at 0x1609a07c0>,\n",
       " <rubicon_ml.client.experiment.Experiment at 0x1609a18a0>,\n",
       " <rubicon_ml.client.experiment.Experiment at 0x1609a02b0>,\n",
       " <rubicon_ml.client.experiment.Experiment at 0x1609a1600>,\n",
       " <rubicon_ml.client.experiment.Experiment at 0x1609a0970>,\n",
       " <rubicon_ml.client.experiment.Experiment at 0x1609a1360>,\n",
       " <rubicon_ml.client.experiment.Experiment at 0x1609a0eb0>,\n",
       " <rubicon_ml.client.experiment.Experiment at 0x1609a1300>,\n",
       " <rubicon_ml.client.experiment.Experiment at 0x1609a0520>,\n",
       " <rubicon_ml.client.experiment.Experiment at 0x1609a00a0>,\n",
       " <rubicon_ml.client.experiment.Experiment at 0x1609a0b50>,\n",
       " <rubicon_ml.client.experiment.Experiment at 0x1609a0d60>,\n",
       " <rubicon_ml.client.experiment.Experiment at 0x1609a11e0>,\n",
       " <rubicon_ml.client.experiment.Experiment at 0x1609a0460>,\n",
       " <rubicon_ml.client.experiment.Experiment at 0x1609a1900>,\n",
       " <rubicon_ml.client.experiment.Experiment at 0x1609a0250>,\n",
       " <rubicon_ml.client.experiment.Experiment at 0x1609a0790>,\n",
       " <rubicon_ml.client.experiment.Experiment at 0x1609a1330>,\n",
       " <rubicon_ml.client.experiment.Experiment at 0x1609a10f0>,\n",
       " <rubicon_ml.client.experiment.Experiment at 0x1609a2350>,\n",
       " <rubicon_ml.client.experiment.Experiment at 0x1609a0a30>,\n",
       " <rubicon_ml.client.experiment.Experiment at 0x1609a2b00>,\n",
       " <rubicon_ml.client.experiment.Experiment at 0x1609a2b90>,\n",
       " <rubicon_ml.client.experiment.Experiment at 0x1609a2470>,\n",
       " <rubicon_ml.client.experiment.Experiment at 0x1609a24a0>,\n",
       " <rubicon_ml.client.experiment.Experiment at 0x1609a07f0>,\n",
       " <rubicon_ml.client.experiment.Experiment at 0x1609a27d0>,\n",
       " <rubicon_ml.client.experiment.Experiment at 0x1609a0220>,\n",
       " <rubicon_ml.client.experiment.Experiment at 0x1609a2530>,\n",
       " <rubicon_ml.client.experiment.Experiment at 0x1609a2650>,\n",
       " <rubicon_ml.client.experiment.Experiment at 0x1609a2830>,\n",
       " <rubicon_ml.client.experiment.Experiment at 0x1609a2e60>,\n",
       " <rubicon_ml.client.experiment.Experiment at 0x1609a2590>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = {\n",
    "    \"si__strategy\": [\"mean\", \"median\", \"most_frequent\"],\n",
    "    \"kn__n_neighbors\": [2, 4, 8, 16, 32, 64],\n",
    "}\n",
    "\n",
    "grid_search_project = rubicon.get_or_create_project(name=\"grid search demo\")\n",
    "rubicon_penguin_pipeline.project = grid_search_project\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    rubicon_penguin_pipeline,\n",
    "    cv=2,\n",
    "    param_grid=parameters,\n",
    "    refit=False,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "grid_search_project.experiments()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64a7b5d-e664-459e-ab52-3f94751aa30d",
   "metadata": {},
   "source": [
    "## widgets & visualizations\n",
    "\n",
    "In a real-world scenario, many modelers will end up with hundreds, thousands or even more experiments.\n",
    "Inspecting this many experiments via a Python interpreter can get messy quickly, so we designed a suite\n",
    "of widgets that can run in-line in a notebook or on their own server to make exploring logged experiments\n",
    "simpler!\n",
    "\n",
    "All of the available visualizations can be found [here](https://capitalone.github.io/rubicon-ml/visualizations.html)\n",
    "and we're always open to requests for new ones! The experiment table rendered in the following cell is\n",
    "a simple overview of each of the experiments logged to a project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e265634c-7b69-4c3e-9b97-1b0e75c7c9e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      " * Serving Flask app 'rubicon_ml.viz.base' (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "data": {},
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rubicon_ml.viz import ExperimentsTable\n",
    "\n",
    "ExperimentsTable(experiments=grid_search_project.experiments()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37001890-5769-4733-b010-a1130fbaa24f",
   "metadata": {},
   "source": [
    "If you're following along with the demo live, make sure to select and publish a few experiments\n",
    "from the experiment table above - it'll be necessary to continue executing the demo.\n",
    "\n",
    "`rubicon-ml` experiments can be published programatically as well. If you did not load the widget,\n",
    "you can publish the experiments like this:\n",
    "\n",
    "```python\n",
    "from rubicon_ml import publish\n",
    "\n",
    "publish(grid_search_project.experiments(), \"./rubicon-ml-catalog.yml\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1590b0-ae8f-4a97-8a96-50645eea99e0",
   "metadata": {},
   "source": [
    "## sharing & collaborating\n",
    "\n",
    "Sharing results and collaborating with others to draw better conclusions is a huge part\n",
    "of the experimentation phase. Sometimes these collaborators may be just as technical as\n",
    "the model developer, or they could be analysts from a completely different domain. The\n",
    "goal of `rubicon-ml`'s `intake` integration is to make sharing `rubicon-ml` experiments\n",
    "easy regardless of the viewers technical skill level.\n",
    "\n",
    "`intake` catalogs can be read with a single call to the `intake` library and return fully\n",
    "operational `rubicon-ml` experiment objects. Future plans include exposing this capability\n",
    "via a simple CLI so users don't even need to launch a Python interpreter. Soon we'll be\n",
    "able to directly launch visualizations from catalog files too!\n",
    "\n",
    "The nice thing about the `intake` catalogs is that its much easier to share experiments\n",
    "by sharing a path to a single file than it is to tell people _\"you need experiments \n",
    "3b6fe9b7-a547-40d4-82ba-e81dd02b1bfb and d1ddfc75-cefb-46ba-b4e4-2cd8e045d7d0 from\n",
    "that S3 bucket we always use...\"_ All that info is succinctly captured in the generated\n",
    "catalog, ready for `intake` to interpret and turn back into `rubicon-ml` experiments.\n",
    "\n",
    "Access controls are handled by the underlying filesystems - as long as the experimenter\n",
    "and collaborator both have access, they'll both be able to share experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db0170f6-fb28-4ea3-9e04-9fb0eada6c3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('4d0029fc-fa04-4907-95ef-46a04ac9b62d', 0.7),\n",
       " ('706a27a5-2ef6-4a5b-89e1-ead0060b4660', 0.725),\n",
       " ('786a9813-d7eb-4e94-9d37-642a27540a45', 0.7)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import intake\n",
    "\n",
    "catalog = intake.open_catalog(\"./rubicon-ml-catalog.yml\")\n",
    "\n",
    "for source in catalog:\n",
    "    catalog[source].discover()\n",
    "    \n",
    "shared_experiments = [catalog[source].read() for source in catalog]\n",
    "\n",
    "[(e.id, e.metric(name=\"score\").value) for e in shared_experiments]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fae9c9-6df0-4590-b3a6-7f28fb97620c",
   "metadata": {},
   "source": [
    "## trying a new model\n",
    "\n",
    "Now, imagine we're a model developer that's collaborating with the old version of us\n",
    "that just trained the k-nearest neighbors model. We've had the best k-nearest neighbor\n",
    "models shared with us via the `intake` catalog above and we think we can do better with\n",
    "a random forest classifier.\n",
    "\n",
    "Similarly to how we did it above, we'll train a new model and log some new experiments\n",
    "to a new `rubicon-ml` project in a completely different location. This time our grid search\n",
    "will log 30 experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "57898299-aaed-4a3b-9e94-ce8d6569fed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 15 candidates, totalling 30 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=2,\n",
       "             estimator=RubiconPipeline(experiment_kwargs={'name': 'RandomForestClassifier',\n",
       "                                                          'tags': ['rf']},\n",
       "                                       project=<rubicon_ml.client.project.Project object at 0x16189a7d0>,\n",
       "                                       steps=[('si', SimpleImputer()),\n",
       "                                              ('rf',\n",
       "                                               RandomForestClassifier())]),\n",
       "             param_grid={'rf__n_estimators': [25, 50, 100, 200, 400],\n",
       "                         'si__strategy': ['mean', 'median', 'most_frequent']},\n",
       "             refit=False, verbose=True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "new_steps = [\n",
    "    (\"si\", SimpleImputer(strategy=\"mean\")),\n",
    "    (\"rf\", RandomForestClassifier(n_estimators=100)),\n",
    "]\n",
    "\n",
    "new_rubicon = Rubicon(\n",
    "    persistence=\"filesystem\",\n",
    "    root_dir=\"./new-rubicon-root\",\n",
    "    auto_git_enabled=True,\n",
    ")\n",
    "new_project = new_rubicon.get_or_create_project(name=\"demo\")\n",
    "\n",
    "new_pipeline = RubiconPipeline(\n",
    "    project=new_project,\n",
    "    experiment_kwargs={\"name\": \"RandomForestClassifier\", \"tags\": [\"rf\"]},\n",
    "    steps=new_steps,\n",
    ")\n",
    "\n",
    "new_parameters = {\n",
    "    \"si__strategy\": [\"mean\", \"median\", \"most_frequent\"],\n",
    "    \"rf__n_estimators\": [25, 50, 100, 200, 400],\n",
    "}\n",
    "\n",
    "new_grid_search = GridSearchCV(\n",
    "    new_pipeline,\n",
    "    cv=2,\n",
    "    param_grid=new_parameters,\n",
    "    refit=False,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "new_grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9b7e41-5846-4fdb-8f7a-c3e67ececa9b",
   "metadata": {},
   "source": [
    "Let's use `rubicon-ml`'s `Dashboard` to inspect these new experiments. The dashboard is a\n",
    "way to combine `rubicon-ml` widgets to create custom dashboards for model interpretability.\n",
    "\n",
    "You can read more about the dashboard [here](https://capitalone.github.io/rubicon-ml/visualizations/dashboard.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "46e9fe23-dbbd-43d1-ac16-781dfd9d96fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dash is running on http://127.0.0.1:8051/\n",
      "\n",
      " * Serving Flask app 'rubicon_ml.viz.base' (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: off\n"
     ]
    }
   ],
   "source": [
    "from rubicon_ml.viz import Dashboard, MetricCorrelationPlot\n",
    "\n",
    "Dashboard(\n",
    "    experiments=new_project.experiments(),\n",
    "    widgets=[\n",
    "        [ExperimentsTable()],\n",
    "        [MetricCorrelationPlot(parameter_names=[\"si__strategy\", \"rf__n_estimators\"])],\n",
    "    ],\n",
    ").serve()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d460f73-48c3-4499-93e8-9cbbb2ba1ba8",
   "metadata": {},
   "source": [
    "We can even publish catalogs containing experiments from completely different `rubicon-ml` \n",
    "locations. This is particularly useful when logging and retrieving experiments from S3.\n",
    "\n",
    "Inspecting the \"combined-catalog.yml\" file, we can see that there are experiments from\n",
    "multiple sources. When we read the whole catalog, we'll get a list of experiments as if\n",
    "they were all loaded from the same place!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "69e1919b-34bc-4559-9be0-d9071f37a4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rubicon_ml import publish\n",
    "\n",
    "combined_catalog = publish(\n",
    "    shared_experiments + new_project.experiments(),\n",
    "    \"./combined-catalog.yml\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae9dc80-593f-42b3-9fe2-a52e5529d7b2",
   "metadata": {},
   "source": [
    "## custom estimators and loggers\n",
    "\n",
    "Say we have some custom code that doesn't fit the Scikit-learn estimator specification,\n",
    "but we'd like to leverage `rubicon-ml`'s automatic logging instead of adding logging\n",
    "statements all over your codebase like the very first example in this notebook.\n",
    "\n",
    "Maybe we decided that both the k-nearest neighbors and random forrest classifiers have\n",
    "some value and should be considered in the final output, so we score predictions with\n",
    "a block of code like this:\n",
    "\n",
    "```python\n",
    "def combined_score(X):\n",
    "    knn_score = KNeighborsClassifier(n_neighbors=n_neighbors).score(X)\n",
    "    rf_score = RandomForestClassifier(n_estimators=n_estimators).score(X)\n",
    "\n",
    "    return (knn_score + (rf_score * 2)) / 3\n",
    "```\n",
    "\n",
    "Instead of manually logging these results to `rubicon-ml`, we can wrap the code\n",
    "above in a class that fits the Scikit-learn estimator specification!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "47d3e50c-357f-441e-9725-989818defc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class ComboEstimator(BaseEstimator):\n",
    "    def __init__(self, n_neighbors=2, n_estimators=25):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.n_estimators = n_estimators\n",
    "        \n",
    "        self.knn = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
    "        self.rf = RandomForestClassifier(n_estimators=n_estimators)\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.knn.fit(X, y)\n",
    "        self.rf.fit(X, y)\n",
    "        \n",
    "    def score(self, X):\n",
    "        knn_score = self.knn.score(X)\n",
    "        rf_score = self.rf.score(X)\n",
    "        \n",
    "        return (knn_score + (rf_score * 2)) / 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c80f06-1191-4634-a3d6-880247d4bfce",
   "metadata": {},
   "source": [
    "Now, all the parameters of the new estimator will be automatically logged when it's\n",
    "placed in a `RubiconPipeline`!\n",
    "\n",
    "If we want to get into custom loggers that can do more than just logging paramters,\n",
    "we'll need to extend `rubicon-ml`'s `EstimatorLogger`. Here, we'll create a new\n",
    "`ModelLogger` that pickles and logs the trained models to `rubicon-ml` as artifacts,\n",
    "or arbitrary files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "209d9523-9ba1-4da4-af15-21b75f779f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "from rubicon_ml.sklearn.estimator_logger import EstimatorLogger\n",
    "\n",
    "class ModelLogger(EstimatorLogger):\n",
    "    def log_parameters(self):\n",
    "        super().log_parameters()\n",
    "        \n",
    "        self.experiment.log_artifact(data_bytes=pickle.dumps(self.estimator.knn), name=\"knn\")\n",
    "        self.experiment.log_artifact(data_bytes=pickle.dumps(self.estimator.rf), name=\"rf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc76332-c083-4613-92fd-757dd7933027",
   "metadata": {},
   "source": [
    "We can pass the new `ComboEstimator` in a tuple along with its custom logger we just\n",
    "implemented above to `rubicon-ml`'s `make_pipeline` function. Like the `RubiconPipeline`,\n",
    "`rubicon-ml`'s version of `make_pipeline` should function very similarly to the Scikit-learn\n",
    "version. The main difference is that it accepts a project as well as optional custom loggers.\n",
    "\n",
    "After we've fit our pipeline, we can retrieve the logged artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b0e13c24-1d8c-434c-8e36-3bcacf1bb15a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('knn', <rubicon_ml.client.artifact.Artifact at 0x161c046a0>),\n",
       " ('rf', <rubicon_ml.client.artifact.Artifact at 0x161c04a00>)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rubicon_ml.sklearn import make_pipeline\n",
    "\n",
    "another_pipeline = make_pipeline(\n",
    "    new_project,\n",
    "    SimpleImputer(strategy=\"mean\"),\n",
    "    (ComboEstimator(n_neighbors=16, n_estimators=100), ModelLogger()),\n",
    ")\n",
    "\n",
    "another_pipeline.fit(X_train, y_train)\n",
    "\n",
    "[(a.name, a) for a in another_pipeline.experiment.artifacts()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a0c13b-9e45-4405-a708-2f1220f4244e",
   "metadata": {},
   "source": [
    "The artifacts' data can be un-pickled to retrieve the trained model objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "86828551-b034-4212-964c-86b2aedd6512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier(n_neighbors=16)\n",
      "RandomForestClassifier()\n"
     ]
    }
   ],
   "source": [
    "for artifact in another_pipeline.experiment.artifacts():\n",
    "    print(pickle.loads(artifact.data))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
