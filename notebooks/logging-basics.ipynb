{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logging Basics\n",
    "\n",
    "Let's see how `rubicon` can help us create an optimal ML model. We'll use supervised learning to train a classification model using `scikit-learn`'s [Iris dataset](https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html) that can predict the class of Iris.\n",
    "\n",
    "Let's load the Iris dataset and take a quick look at it's contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "print(iris['DESCR'][:193])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll load the data into a dask dataframe so it's easier to work with throughout our example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask import dataframe as dd\n",
    "\n",
    "iris_data = dd.from_array(iris.data, columns=iris.feature_names)\n",
    "iris_data.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each sample (row in our dataframe) holds measurements of one of the 3 classes of Iris plants. We'll train on 75% of the data and use the last 25% to make predictions using our model. Then, we'll compare with the recorded results to compute our model's accuracy.\n",
    "\n",
    "Let's shuffle the data and define our training and prediction subsets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "random_state = int(datetime.utcnow().timestamp())\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris['data'], iris['target'], random_state=random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a method for fitting our model. We'll use sklearn's [Random Forest Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def fit_classifier(n_estimators, n_features, random_state, X_train, y_train):\n",
    "    rfc = RandomForestClassifier(n_estimators=n_estimators, random_state=random_state)\n",
    "    rfc.fit(X_train, y_train)\n",
    "    \n",
    "    return rfc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RandomForestClassifier accepts `n_estimators` (the number of trees in the forest) as a **parameter**. What if we wanted to tune this **parameter** to see how it affected our output **metric**, accuracy?\n",
    "\n",
    "This is where `Rubicon`'s **experiment** logging comes in handy. Let's get `Rubicon` setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rubicon import Rubicon\n",
    "\n",
    "root_dir = \"./rubicon-root\"\n",
    "rubicon = Rubicon(persistence=\"filesystem\", root_dir=root_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's create a project to hold our various **experiments** as we try different values of `n_estimators`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project = rubicon.create_project(\"Iris Model\", description=\"Using scikit-learn's to create an Iris classifier.\")\n",
    "print(project)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's log some **experiments** and tag them with \"success\" if their accuracy is greater than or equal to 94% (arbitrarily chosen for this example) so we can easily retrieve them later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "SklearnTrainingMetadata = namedtuple(\"SklearnTrainingMetadata\", \"module_name method\")\n",
    "\n",
    "for e in range(1, 100, 5):\n",
    "    n_estimators = e\n",
    "    n_features = len(iris.feature_names)\n",
    "    \n",
    "    experiment = project.log_experiment(\n",
    "        training_metadata=[SklearnTrainingMetadata(\"sklearn.datasets\", \"load_iris\")],\n",
    "        model_name=\"Iris Prediction Model\",\n",
    "        tags=[\"iris\"],\n",
    "    )\n",
    "    \n",
    "    experiment.log_parameter(\"n_estimators\", n_estimators)\n",
    "    experiment.log_parameter(\"n_features\", n_features)\n",
    "    experiment.log_parameter(\"random_state\", random_state)\n",
    "    \n",
    "    rfc = fit_classifier(n_estimators, n_features, random_state, X_train, y_train)\n",
    "    \n",
    "    feature_importances = list(zip(iris.feature_names, rfc.feature_importances_))\n",
    "    for name, importance in feature_importances:\n",
    "        experiment.log_feature(name, importance=importance)\n",
    "    \n",
    "    accuracy = rfc.score(X_test, y_test)\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    \n",
    "    experiment.log_metric(\"accuracy\", accuracy)\n",
    "    \n",
    "    if accuracy >= .94:\n",
    "        experiment.add_tags([\"success\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fetch the **experiments** tagged with \"success\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "success_experiments = project.experiments(tags=[\"success\"])\n",
    "for success_experiment in success_experiments:\n",
    "    for p in success_experiment.parameters():\n",
    "        if p.name == 'n_estimators':\n",
    "            print(f\"n_estimators value: {p.value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this is a simple example, it shows how we can use `rubicon` to track our model's performance over time as we try different **parameters** to optimize our **metrics**. We also logged the training metadata, **parameters**, and **features** used for each **experiment** so we can analyze that data later on.\n",
    "\n",
    "`rubicon` supports even more logging capabilities, like logging **artifacts** and **dataframes**, to ensure complete reproducibility as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "Now that we're \"done\" logging our data, wew'd likely want to explore it. We could use Rubicon's Dashboard to visualize our logged data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rubicon.ui import Dashboard\n",
    "\n",
    "Dashboard(persistence=\"filesystem\", root_dir=\"./rubicon-root\").run_server()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or we could grab the project's data as a pandas dataframe, which could easily be manipulated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = rubicon.get_project_as_dask_df(\"Iris Model\")\n",
    "ddf.compute()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
