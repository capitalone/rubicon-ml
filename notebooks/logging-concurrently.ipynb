{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logging Concurrently\n",
    "\n",
    "Once we've got the hang of using `rubicon`, we can expand on our project from the *Iris Classifier* example. \n",
    "Let's see how a few other popular `scikit-learn` models perform with the Iris dataset. `rubicon` logging is totally \n",
    "thread-safe, so we can test a lot of model configurations at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rubicon import Rubicon\n",
    "\n",
    "root_dir = \"./rubicon-root\"\n",
    "\n",
    "rubicon = Rubicon(persistence=\"filesystem\", root_dir=root_dir)\n",
    "project = rubicon.create_project(\"Concurrent Experiments\", description=\"Training multiple models in parallel\")\n",
    "\n",
    "print(project)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a recap of the contents of the Iris dataset, check out `iris.DESCR` and `iris.data`. We'll put together\n",
    "a training dataset using a subset of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "random_state = int(datetime.utcnow().timestamp())\n",
    "\n",
    "iris = load_iris()\n",
    "iris_datasets = train_test_split(iris['data'], iris['target'], random_state=random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use `run_experiment` to log a new experiment to the provided `project` then train, run and log a model of type\n",
    "`classifier_cls` using the training and testing data in `iris_datasets`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import namedtuple\n",
    "\n",
    "SklearnTrainingMetadata = namedtuple(\"SklearnTrainingMetadata\", \"module_name method\")\n",
    "\n",
    "def run_experiment(project, classifier_cls, iris_datasets, **kwargs):\n",
    "    X_train, X_test, y_train, y_test = iris_datasets\n",
    "    \n",
    "    experiment = project.log_experiment(\n",
    "        training_metadata=[SklearnTrainingMetadata(\"sklearn.datasets\", \"load_iris\")],\n",
    "        model_name=classifier_cls.__name__,\n",
    "        tags=[classifier_cls.__name__],\n",
    "    )\n",
    "    \n",
    "    for key, value in kwargs.items():\n",
    "        experiment.log_parameter(key, value)\n",
    "    \n",
    "    n_features = len(iris.feature_names)\n",
    "    experiment.log_parameter(\"n_features\", n_features)\n",
    "    \n",
    "    for name in iris.feature_names:\n",
    "        experiment.log_feature(name)\n",
    "        \n",
    "    classifier = classifier_cls(**kwargs)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    classifier.predict(X_test)\n",
    "    \n",
    "    accuracy = classifier.score(X_test, y_test)\n",
    "    \n",
    "    experiment.log_metric(\"accuracy\", accuracy)\n",
    "\n",
    "    if accuracy >= .95:\n",
    "        experiment.add_tags([\"success\"])\n",
    "    else:\n",
    "        experiment.add_tags([\"failure\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we'll take a look at two more classifiers, **decision tree** and **k-neighbors**, in addition to the **random forest** classifier we used in the last example. Each classifier will be ran across four sets of parameters (provided as `kwargs` to `run_experiment`), for a total of 12 experiments. Here, we'll build up a list of processes that will run each experiment in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "processes = []\n",
    "\n",
    "for n_estimators in [10, 20, 30, 40]:\n",
    "    processes.append(multiprocessing.Process(\n",
    "        target=run_experiment, args=[project, RandomForestClassifier, iris_datasets],\n",
    "        kwargs={\"n_estimators\": n_estimators, \"random_state\": random_state},\n",
    "    ))\n",
    "    \n",
    "for criterion in [\"gini\", \"entropy\"]:\n",
    "    for splitter in [\"best\", \"random\"]:\n",
    "        processes.append(multiprocessing.Process(\n",
    "            target=run_experiment, args=[project, DecisionTreeClassifier, iris_datasets],\n",
    "            kwargs={\"criterion\": criterion, \"splitter\": splitter, \"random_state\": random_state},\n",
    "        ))\n",
    "\n",
    "for n_neighbors in [5, 10, 15, 20]:\n",
    "    processes.append(multiprocessing.Process(\n",
    "        target=run_experiment, args=[project, KNeighborsClassifier, iris_datasets],\n",
    "        kwargs={\"n_neighbors\": n_neighbors},\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run all our experiments in parallel!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for process in processes:\n",
    "    process.start()\n",
    "    \n",
    "for process in processes:\n",
    "    process.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can validate that we successfully logged all 12 experiments to our project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(project.experiments())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see which experiments we tagged as successful and what type of model they used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in project.experiments(tags=[\"success\"]):    \n",
    "    print(f\"experiment {e.id} was successful using a {e.model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also take a deeper look at any of our experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = project.experiments()[0]\n",
    "\n",
    "print(f\"training_metadata: {SklearnTrainingMetadata(*experiment.training_metadata)}\")\n",
    "print(f\"tags: {experiment.tags}\")\n",
    "print(\"parameters:\")\n",
    "for parameter in experiment.parameters():\n",
    "    print(f\"\\t{parameter.name}: {parameter.value}\")\n",
    "print(\"features:\")\n",
    "for feature in experiment.features():\n",
    "    print(f\"\\t{feature.name}\")\n",
    "print(\"metrics:\")\n",
    "for metric in experiment.metrics():\n",
    "    print(f\"\\t{metric.name}: {metric.value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model developers can take advantage of `rubicon`'s thread-safety to test tons of models at once and collect results\n",
    "in a standardized format to analyze which ones performed the best. `rubicon` can even be run in more advanced\n",
    "distributed setups, like on a *Dask* cluster."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "nbsphinx": {
   "execute": "always"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
