{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b466d92",
   "metadata": {},
   "source": [
    "## Distinguishing Failed Experiments with Rubicon \n",
    "When logging with `rubicon_ml` , if an experiment fails, it will result in an empty experiment. In this example, we'll walk through how to handle these experiments. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f58990",
   "metadata": {},
   "source": [
    "First lets simulate the problem. To do this we'll create an estimator that will fail on it's `fit()` by throwing a `RuntimeError`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9a506c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class BadEstimator(BaseEstimator):\n",
    "    def __init__(self, n_neighbors=2):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_neighbors = n_neighbors\n",
    "        \n",
    "        self.knn = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
    "    def fit(self, X, y):\n",
    "        self.knn.fit(X, y)\n",
    "        try:\n",
    "            raise RuntimeError(\"Bad Estimator\")\n",
    "        except RuntimeError:\n",
    "            pass\n",
    "        \n",
    "    def score(self, X):\n",
    "        knn_score = self.knn.score(X)\n",
    "        return knn_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d62eaa1",
   "metadata": {},
   "source": [
    "Now let's attempt to create a `rubicon_ml.sklearn` pipeline with this failing estimator and attempt to `fit` the pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ccb023c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RubiconPipeline(project=<rubicon_ml.client.project.Project object at 0x16b821f30>,\n",
       "                steps=[('simpleimputer', SimpleImputer()),\n",
       "                       ('badestimator', BadEstimator())])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rubicon_ml.sklearn import make_pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "\n",
    "from rubicon_ml import Rubicon\n",
    "\n",
    "rubicon = Rubicon(\n",
    "    persistence=\"memory\",\n",
    ")\n",
    "project = rubicon.get_or_create_project(name=\"Failed Experiment\")\n",
    "\n",
    "X = [[1], [1], [1], [1]]\n",
    "y = [1, 1, 1, 1]\n",
    "rf=RandomForestClassifier(n_estimators=50)\n",
    "exp=project.log_experiment()\n",
    "pipe=make_pipeline(project, SimpleImputer(strategy=\"mean\"),BadEstimator())\n",
    "pipe.fit(X,y, experiment=exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d57176",
   "metadata": {},
   "source": [
    "When an experiment is create we can check its contents for length. Here we can see nothing has been logged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59ec4c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_config': <rubicon_ml.client.config.Config object at 0x16b823040>, '_domain': Experiment(project_name='Failed Experiment', id='f991f8d7-258f-41e7-9455-c5f4041bc718', name=None, description=None, model_name=None, branch_name=None, commit_hash=None, training_metadata=None, tags=[], created_at=datetime.datetime(2022, 5, 2, 15, 37, 40, 715821)), '_parent': <rubicon_ml.client.project.Project object at 0x16b821f30>, '_artifacts': [], '_dataframes': [], '_metrics': [], '_features': [], '_parameters': []}\n"
     ]
    }
   ],
   "source": [
    "exp=project.experiments()[0]\n",
    "print(exp.__dict__)\n",
    "assert len(exp.artifacts())==0\n",
    "assert len(exp.dataframes())==0\n",
    "assert len(exp.metrics())==0\n",
    "assert len(exp.features())==0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb4ac2c",
   "metadata": {},
   "source": [
    "Since we have observed an empty experiment, we need to identify it as a failed experiment. We can do this by leveraging the `tags` attribute for the experiment with `add_tags()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3dd9b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.add_tags([\"failed\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e03bd0a",
   "metadata": {},
   "source": [
    "Finally, we can no retrieve all our failed experiments by passing the `tags=[\"failed\"]` to `project.experiments()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ef73ac0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<rubicon_ml.client.experiment.Experiment at 0x16b821de0>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project.experiments(tags=[\"failed\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
