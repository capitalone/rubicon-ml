{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logging Asynchronously\n",
    "\n",
    "The asynchronous `rubicon` client offers a way to read and write `rubicon` objects using Python's builtin `asyncio` module. `rubicon` is lightweight computationally, but reading and writing to S3 takes time over the network. We can use `asyncio` to asynchronously communicate with S3 while executing other work.\n",
    "\n",
    "There are **two main differences** between the asynchronous and standard `rubicon` clients:\n",
    "\n",
    "* the asynchronous client is for **S3 logging only**\n",
    "* the asynchronous client's functions **return coroutines** rather than their standard return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rubicon.client.asynchronous import Rubicon\n",
    "\n",
    "s3_bucket = \"my-bucket\"\n",
    "root_dir = f\"s3://{s3_bucket}/rubicon-root\"\n",
    "\n",
    "rubicon = Rubicon(persistence=\"filesystem\", root_dir=root_dir)\n",
    "project = await rubicon.get_or_create_project(\n",
    "    \"Asynchronous Experiments\", description=\"Training multiple models asynchronously\"\n",
    ")\n",
    "\n",
    "print(project)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll take another look at the Iris dataset in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "random_state = int(datetime.utcnow().timestamp())\n",
    "\n",
    "iris = load_iris()\n",
    "iris_datasets = train_test_split(iris['data'], iris['target'], random_state=random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define an asynchronous `run_experiment` function to log a new experiment to the provided `project` then \n",
    "train, run and log a model of type `classifier_cls` using the training and testing data in `iris_datasets`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import pandas as pd\n",
    "from collections import namedtuple\n",
    "\n",
    "SklearnTrainingMetadata = namedtuple(\"SklearnTrainingMetadata\", \"module_name method\")\n",
    "\n",
    "async def run_experiment(project, classifier_cls, iris_datasets, **kwargs):\n",
    "    X_train, X_test, y_train, y_test = iris_datasets\n",
    "    \n",
    "    # await logging the experiment so we can log other things to it\n",
    "    experiment = await project.log_experiment(\n",
    "        training_metadata=[SklearnTrainingMetadata(\"sklearn.datasets\", \"load_iris\")],\n",
    "        model_name=classifier_cls.__name__,\n",
    "        tags=[classifier_cls.__name__],\n",
    "    )\n",
    "    \n",
    "    # gather a list of coroutines that will log objects to the experiment\n",
    "    rubicon_logging_coroutines = []\n",
    "    \n",
    "    for key, value in kwargs.items():\n",
    "        parameter_coroutine = experiment.log_parameter(key, value)\n",
    "        rubicon_logging_coroutines.append(parameter_coroutine)\n",
    "    \n",
    "    parameter_coroutine = experiment.log_parameter(\"n_features\", len(iris.feature_names))\n",
    "    rubicon_logging_coroutines.append(parameter_coroutine)\n",
    "    \n",
    "    for name in iris.feature_names:\n",
    "        feature_coroutine = experiment.log_feature(name)\n",
    "        rubicon_logging_coroutines.append(feature_coroutine)\n",
    "        \n",
    "    classifier = classifier_cls(**kwargs)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    classifier.predict(X_test)\n",
    "    \n",
    "    accuracy = classifier.score(X_test, y_test)\n",
    "    \n",
    "    metric_coroutine = experiment.log_metric(\"accuracy\", accuracy)\n",
    "    rubicon_logging_coroutines.append(metric_coroutine)\n",
    "\n",
    "    if accuracy >= .94:\n",
    "        tag_coroutine = experiment.add_tags([\"success\"])\n",
    "    else:\n",
    "        tag_coroutine = experiment.add_tags([\"failure\"])\n",
    "    rubicon_logging_coroutines.append(tag_coroutine)\n",
    "    \n",
    "    # execute all logging coroutines asynchronously\n",
    "    await asyncio.gather(*rubicon_logging_coroutines)\n",
    "    \n",
    "    return experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we'll take a look at two more classifiers, **decision tree** and **k-neighbors**, in addition to the **random forest** classifier. Each classifier will be ran across four sets of parameters (provided as `kwargs` to `run_experiment`), for a total of 12 experiments. Here, we'll build up a list of coroutines that will run each experiment asynchronously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "coroutines = []\n",
    "\n",
    "for n_estimators in [10, 20, 30, 40]:\n",
    "    coroutines.append(run_experiment(\n",
    "        project, RandomForestClassifier, iris_datasets,\n",
    "        n_estimators=n_estimators, random_state=random_state,\n",
    "    ))\n",
    "    \n",
    "for criterion in [\"gini\", \"entropy\"]:\n",
    "    for splitter in [\"best\", \"random\"]:\n",
    "        coroutines.append(run_experiment(\n",
    "            project, DecisionTreeClassifier, iris_datasets,\n",
    "            criterion=criterion, splitter=splitter, random_state=random_state,\n",
    "        ))\n",
    "\n",
    "for n_neighbors in [5, 10, 15, 20]:\n",
    "    coroutines.append(run_experiment(\n",
    "        project, KNeighborsClassifier, iris_datasets, n_neighbors=n_neighbors,\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run all our experiments asynchronously!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = await asyncio.gather(*coroutines)\n",
    "experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can validate that we successfully logged all 12 experiments to our project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(await project.experiments())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see which experiments we tagged as successful and what type of model they used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in await project.experiments(tags=[\"success\"]):    \n",
    "    print(f\"experiment {e.id} was successful using a {e.model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also take a deeper look at any of our experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = experiments[0]\n",
    "\n",
    "print(f\"training_metadata: {SklearnTrainingMetadata(*experiment.training_metadata)}\")\n",
    "print(f\"tags: {await experiment.tags}\")\n",
    "print(\"parameters:\")\n",
    "for parameter in await experiment.parameters():\n",
    "    print(f\"\\t{parameter.name}: {parameter.value}\")\n",
    "print(\"features:\")\n",
    "for feature in await experiment.features():\n",
    "    print(f\"\\t{feature.name}\")\n",
    "print(\"metrics:\")\n",
    "for metric in await experiment.metrics():\n",
    "    print(f\"\\t{metric.name}: {metric.value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we could grab the project's data as a dataframe, which could easily be manipulated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = await rubicon.get_project_as_dask_df(\"Asynchronous Experiments\")\n",
    "ddf.compute()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
