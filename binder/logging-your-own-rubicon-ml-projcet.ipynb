{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa876934-931b-4d5c-b8bd-213e2bc5adab",
   "metadata": {},
   "source": [
    "# Logging Your Own `rubicon-ml` Project\n",
    "\n",
    "`rubicon-ml` plays nicely with multiple popular projects in the PyData ecosystem. If you're already\n",
    "using Scikit-learn, logging with `rubicon-ml` is simple! The code in the cell below is a copy of\n",
    "[this Scikit-learn example](https://scikit-learn.org/stable/auto_examples/model_selection/grid_search_text_feature_extraction.html#sphx-glr-auto-examples-model-selection-grid-search-text-feature-extraction-py)\n",
    "from their documentation. You'll create a `rubicon-ml` project, change one line in the cell below,\n",
    "and you'll be logging!\n",
    "\n",
    "You don't need to run this cell before adding in `rubicon-ml`. Jump down to the next one to see how!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66378815-559d-4708-a4e9-ddbbca9147bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Olivier Grisel <olivier.grisel@ensta.org>\n",
    "#         Peter Prettenhofer <peter.prettenhofer@gmail.com>\n",
    "#         Mathieu Blondel <mathieu@mblondel.org>\n",
    "# License: BSD 3 clause\n",
    "from pprint import pprint\n",
    "from time import time\n",
    "import logging\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "print(__doc__)\n",
    "\n",
    "# Display progress logs on stdout\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s %(levelname)s %(message)s')\n",
    "\n",
    "\n",
    "# #############################################################################\n",
    "# Load some categories from the training set\n",
    "categories = [\n",
    "    'alt.atheism',\n",
    "    'talk.religion.misc',\n",
    "]\n",
    "# Uncomment the following to do the analysis on all the categories\n",
    "#categories = None\n",
    "\n",
    "print(\"Loading 20 newsgroups dataset for categories:\")\n",
    "print(categories)\n",
    "\n",
    "data = fetch_20newsgroups(subset='train', categories=categories)\n",
    "print(\"%d documents\" % len(data.filenames))\n",
    "print(\"%d categories\" % len(data.target_names))\n",
    "print()\n",
    "\n",
    "# #############################################################################\n",
    "# Define a pipeline combining a text feature extractor with a simple\n",
    "# classifier\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', SGDClassifier()),\n",
    "])\n",
    "\n",
    "# uncommenting more parameters will give better exploring power but will\n",
    "# increase processing time in a combinatorial way\n",
    "parameters = {\n",
    "    'vect__max_df': (0.5, 0.75, 1.0),\n",
    "    # 'vect__max_features': (None, 5000, 10000, 50000),\n",
    "    'vect__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams\n",
    "    # 'tfidf__use_idf': (True, False),\n",
    "    # 'tfidf__norm': ('l1', 'l2'),\n",
    "    'clf__max_iter': (20,),\n",
    "    'clf__alpha': (0.00001, 0.000001),\n",
    "    'clf__penalty': ('l2', 'elasticnet'),\n",
    "    # 'clf__max_iter': (10, 50, 80),\n",
    "}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # multiprocessing requires the fork to happen in a __main__ protected\n",
    "    # block\n",
    "\n",
    "    # find the best parameters for both the feature extraction and the\n",
    "    # classifier\n",
    "    grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1)\n",
    "\n",
    "    print(\"Performing grid search...\")\n",
    "    print(\"pipeline:\", [name for name, _ in pipeline.steps])\n",
    "    print(\"parameters:\")\n",
    "    pprint(parameters)\n",
    "    t0 = time()\n",
    "    grid_search.fit(data.data, data.target)\n",
    "    print(\"done in %0.3fs\" % (time() - t0))\n",
    "    print()\n",
    "\n",
    "    print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "    print(\"Best parameters set:\")\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27ff331-7ad4-4dd9-bc1c-ad3808cb7a30",
   "metadata": {},
   "source": [
    "First, you can create a project to store your logged model runs. You'll also import\n",
    "the `RubiconPipeline`, which will replace the standard Scikit-learn pipeline in the\n",
    "example above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afd5a4c-acef-4814-b283-e81aae2b05bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from rubicon import Rubicon\n",
    "from rubicon.sklearn import RubiconPipeline\n",
    "\n",
    "\n",
    "rubicon = Rubicon(persistence=\"filesystem\", root_dir=f\"{os.getcwd()}/rubicon-root\")\n",
    "project = rubicon.get_or_create_project(name=\"Binder Pipeline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef82ca5-6e4e-4026-a6ed-be75a6c5a925",
   "metadata": {},
   "source": [
    "Then you just need to swap the Scikit-learn `Pipeline` with the `RubiconPipeline` and pass\n",
    "in the project you just created as the first parameter. After editing the first cell, your\n",
    "pipeline definition should look like this:\n",
    "\n",
    "\n",
    "```python\n",
    "pipeline = RubiconPipeline(project, [\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', SGDClassifier()),\n",
    "])\n",
    "```\n",
    "\n",
    "When everything looks good, give the first cell a run to log some data! Once it's finished,\n",
    "you can use the dashboard to explore all the model runs in the **Binder Pipeline** project.\n",
    "This time we'll run the dashboard in a new window within JupyterLab by instantiating the\n",
    "dashboard with `mode=\"jupyterlab\"`.\n",
    "\n",
    "You'll notice the output from the\n",
    "original example states that 120 models were fit - that's one for each of the 24 parameter\n",
    "sets across 5 folds of the data. For each of those 120 fits, you'll see an experiment logged\n",
    "by `rubicon-ml` on the dashboard (You'll also see a 121st experiment without a score that was\n",
    "generated when the model's best estimator was fit at the end of the grid search).\n",
    "\n",
    "These experiments contain a lot of data, as `rubicon-ml`'s\n",
    "Scikit-learn logging is very verbose by default. To learn more about `rubicon-ml`'s\n",
    "Scikit-learn integration and filtering logged estimator parameters, check out the\n",
    "`integration-sklearn` example in `notebooks/integrations`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b29216b-9919-4a0b-82c1-0ec063964ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rubicon.ui import Dashboard\n",
    "\n",
    "\n",
    "Dashboard(\n",
    "    persistence=\"filesystem\",\n",
    "    root_dir=f\"{os.getcwd()}/rubicon-root\",\n",
    "    mode=\"jupyterlab\",\n",
    ").run_server(debug=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
